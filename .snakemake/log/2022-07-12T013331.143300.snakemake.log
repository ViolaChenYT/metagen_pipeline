Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job              count    min threads    max threads
-------------  -------  -------------  -------------
annotate_gene        1              1              1
filter               1              1              1
lofreq               1              1              1
total                3              1              1

Select jobs to execute...

[Tue Jul 12 01:33:31 2022]
rule filter:
    input: /mnt/volume1/cpe/MBH038/ec.bam
    output: /mnt/volume1/cpe/MBH038/ec_filt.bam
    jobid: 2
    wildcards: sample=MBH038
    resources: tmpdir=/tmp

[Tue Jul 12 01:33:31 2022]
Error in rule filter:
    jobid: 2
    output: /mnt/volume1/cpe/MBH038/ec_filt.bam
    shell:
        python /mnt/volume1/scripts/bamfilter.py /mnt/volume1/cpe/MBH038/ec.bam /mnt/volume1/cpe/MBH038/ec_filt.bam
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job filter since they might be corrupted:
/mnt/volume1/cpe/MBH038/ec_filt.bam
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /mnt/volume1/pipeline/.snakemake/log/2022-07-12T013331.143300.snakemake.log
